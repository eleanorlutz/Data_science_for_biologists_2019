{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Clustering in Scikit-Learn\n",
    "**Data Science for Biologists** &#8226; University of Washington &#8226; BIOL 419/519 &#8226; Winter 2019\n",
    "\n",
    "Course design and lecture material by [Bingni Brunton](https://github.com/bwbrunton) and [Kameron Harris](https://github.com/kharris/). Lab design and materials by [Eleanor Lutz](https://github.com/eleanorlutz/), with helpful comments and suggestions from Bing and Kam.\n",
    "\n",
    "### Table of Contents\n",
    "1. K-means clustering using scikit-learn\n",
    "2. Bonus exercises\n",
    "\n",
    "### Helpful resources\n",
    "- [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas\n",
    "- [An introduction to machine learning with Scikit-Learn](https://scikit-learn.org/stable/tutorial/basic/tutorial.html)\n",
    "- [Scikit-Learn user guide](https://scikit-learn.org/stable/user_guide.html)\n",
    "- [Scikit-Learn Cheat Sheet](https://datacamp-community-prod.s3.amazonaws.com/5433fa18-9f43-44cc-b228-74672efcd116) by Python for Data Science\n",
    "\n",
    "### Data\n",
    "- The data in this lab is from the [Palmer Penguin Project](https://github.com/allisonhorst/palmerpenguins) by Dr. Kristen Gorman. The data was edited for teaching purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-colorblind\") # Use a colorblind friendly color scheme\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 7 Part 1: \n",
    "\n",
    "In previous labs, we created models that would either allow us to:\n",
    "\n",
    "- predict one metric from another\n",
    "- classify new data samples based on known categories in the training data\n",
    "\n",
    "Both of these approaches were examples of supervised learning. We fit models using existing training data with the goal of predicting values for new test data.\n",
    "\n",
    "Today, we want to see what patterns emerge from data when we apply the unsupervised learning technique of k-means clustering. This week's data consists of measurements taken from three species of penguins (we looked at this data previously in Lab 2). \n",
    "\n",
    "![Illustration showing the three penguin species](figures/lter_penguins.jpg)\n",
    "\n",
    "*Credit:* Artwork by @allison_horst\n",
    "\n",
    "**Exercise 1:** Read in the `penguin.csv` dataset and display the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/Lab_07/penguin_data.csv\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** Create a scatterplot of culmen length vs culmen depth, with each species plotted as a different color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {'Adelie':0, 'Chinstrap':1, 'Gentoo':2}\n",
    "\n",
    "plt.scatter(df[\"culmen.length\"], df[\"culmen.depth\"], c=df['species'].apply(lambda x: colors[x]))\n",
    "plt.xlabel(\"Culmen length (mm)\")\n",
    "plt.ylabel(\"Culmen depth (mm)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering with sci-kit learn\n",
    "\n",
    "K-means clustering is an iterative clustering algorithm that is used to cluster unlabeled data. For example, the data you have may be unlabeled because it was collected by someone else who didn't have time to label everything by hand.\n",
    "\n",
    "- **Initialization:** First, k-means randomly chooses ${k}$ samples from the data to use as the initial cluster centers (${k}$ is the number of clusters). \n",
    "- **Cluster Assignment:** Next, each data point is assigned to the cluster center that is closest to that data point. \n",
    "- **Move centroid:** At this point each cluster center should have a set of data points associated with that cluster assignment. We will now update the cluster center to be the mean of all of the data points assigned to that cluster. \n",
    "- **Iterate:** The previous step will likely move the cluster centers. If it did, we will repeat the process again and again until the centroids no longer move after the cluster assignment step.\n",
    "\n",
    "Below is an animation showing the steps of K-means clustering for ${k = 4}$\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*4LOxZL6bFl3rXlr2uCiKlQ.gif\" />\n",
    "\n",
    "*Credit:* Andrey A. Shabalin\n",
    "\n",
    "To run the k-means algorithm in Scikit-learn, import `sklearn.cluster.KMeans`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** First, split the dataframe into training and test data sets called `train_data` and `test_data`. Refer to the Lab 6 \"Splitting data using Pandas\" section for a fast way to separate data into training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df.sample(frac=0.7)\n",
    "test_data = df.drop(train_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `KMeans` is an unsupervised learning algorithm, it does not use an answer dataset (like `LDA` from last week). We'll remove the species descriptions from both dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_n = train_data.drop(\"species\", axis=1)\n",
    "test_data_n = test_data.drop(\"species\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use k-means clustering to fit a cluster called `cluster` to your penguin data matrix with $k=2$ clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = KMeans(n_clusters=2)\n",
    "cluster.fit(train_data_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation for [Scikit-learn K-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) describes what can be done after fitting this model. For example, the attribute `labels_` returns an array of the cluster ID for every data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.labels_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:** Create a new column, `labels`, in the `train_data` penguin data frame using the labels from the k-means model. Display the first five rows of the dataframe. Plot culmen length by culmen depth from this dataframe with points colored by the k-means cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"labels\"] = cluster.labels_\n",
    "display(train_data.head())\n",
    "\n",
    "plt.scatter(train_data[\"culmen.length\"], train_data[\"culmen.depth\"], c=train_data['labels'])\n",
    "plt.xlabel(\"Culmen length (mm)\")\n",
    "plt.ylabel(\"Culmen depth (mm)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another helpful Scikit-learn method is `predict`, which uses the previously created clustering algorithm to predict the closest cluster for the given data. We can use this to predict the cluster for a previously unseen data point, or to look at different data points within the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.predict(test_data_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the predicted cluster IDs to the known species names for each sample to see how well the machine learning algorithm matches the species classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'] = cluster.predict(test_data_n)\n",
    "test_data.groupby(['species','label']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:** Cluster your penguin data again using $k=3$ clusters. Display the first five rows of the training dataset. After clustering, replace your original cluster labels in `train_data` with the new labels and plot culmen length by culmen depth (scatterpoints colored by cluster label). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_data_n.head())\n",
    "\n",
    "cluster_5 = KMeans(n_clusters=3)\n",
    "cluster_5.fit(train_data_n)\n",
    "\n",
    "train_data[\"labels\"] = cluster_5.labels_\n",
    "\n",
    "plt.scatter(train_data[\"culmen.length\"], train_data[\"culmen.depth\"], c=train_data['labels'])\n",
    "plt.xlabel(\"Culmen length (mm)\")\n",
    "plt.ylabel(\"Culmen depth (mm)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:** Use this new cluster to predict the cluster ID of the test data. Display the number of samples that belong to each cluster-species group. Does it seem like there is one species that clusters better than the others? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'] = cluster_5.predict(test_data_n)\n",
    "test_data.groupby(['species','label']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:** Cluster only the `culmen.length` and `culmen.depth` of the penguin data matrix into $k=3$ clusters. Display the first five rows of the training dataset. Repeat the plot above. How do the results change with fewer dimensions for the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_n2 = train_data.drop([\"flipper.length\", \"body.mass\", \"species\", \"labels\"], axis=1)\n",
    "display(train_data_n2.head())\n",
    "\n",
    "cluster_6 = KMeans(n_clusters=3)\n",
    "cluster_6.fit(train_data_n2)\n",
    "\n",
    "train_data[\"labels\"] = cluster_6.labels_\n",
    "\n",
    "plt.scatter(train_data[\"culmen.length\"], train_data[\"culmen.depth\"], c=train_data['labels'])\n",
    "plt.xlabel(\"Culmen length (mm)\")\n",
    "plt.ylabel(\"Culmen depth (mm)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 7 Bonus exercise\n",
    "\n",
    "**Bonus Exercise 1:** Visualize the success of the clustering algorithm compared to the true species values. Plot the culmen length, culmen depth, and flipper length in a 3D plot. Each scatterpoint should be colored according to species, and the clustering results should be represented by different marker shapes (circle, X, diamond, etc). *Hint:* Lab 4 includes code for plotting a 3D plot in Bonus Exercise 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# See Lab 4 Answer Key for how to plot in 3D\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "groups = train_data.groupby(\"labels\")\n",
    "colors = {'Adelie':\"C0\", 'Chinstrap':\"C1\", 'Gentoo':\"C2\"}\n",
    "scatters = [\"d\", \"x\", \"^\"]\n",
    "\n",
    "for cluster, group in groups: \n",
    "    scatter = scatters[cluster]\n",
    "    ax.scatter(group[\"culmen.length\"], group[\"culmen.depth\"], group[\"flipper.length\"],\n",
    "               c=group[\"species\"].apply(lambda x: colors[x]), marker=scatter)\n",
    "\n",
    "ax.set_xlabel(\"Culmen length (mm)\")\n",
    "ax.set_ylabel(\"Culmen depth (mm)\")\n",
    "ax.set_zlabel(\"Flipper length (mm)\")\n",
    "\n",
    "# Create a custom legend\n",
    "legend = [matplotlib.lines.Line2D([0], [0], color='C0', marker='o', lw=0, \n",
    "                                  label='Adelie'),\n",
    "          matplotlib.lines.Line2D([0], [0], color='C1', marker='o', lw=0, \n",
    "                                  label='Chinstrap'),\n",
    "          matplotlib.lines.Line2D([0], [0], color='C2', marker='o', lw=0, \n",
    "                                  label='Gentoo'),\n",
    "          matplotlib.lines.Line2D([0], [0], color='k', marker=scatters[0], lw=0, \n",
    "                                  label='Cluster 0'),\n",
    "          matplotlib.lines.Line2D([0], [0], color='k', marker=scatters[1], lw=0, \n",
    "                                  label='Cluster 1'),\n",
    "          matplotlib.lines.Line2D([0], [0], color='k', marker=scatters[2], lw=0, \n",
    "                                  label='Cluster 2'),]\n",
    "ax.legend(handles=legend)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
